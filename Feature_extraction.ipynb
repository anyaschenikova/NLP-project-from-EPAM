{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7mf1dHlG6iH"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T4RPzEjG6iK"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('data_lemm.csv')\n",
        "# data.dropna(inplace=True)\n",
        "# data.reset_index(drop=True, inplace=True)\n",
        "# data.replace('\\d+', '', regex=True, inplace=True)\n",
        "# data.replace(\"^a-zA-Z0-9]+\", \"\", regex=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za3HPu3QG6iL"
      },
      "outputs": [],
      "source": [
        "# data['class'] = 0\n",
        "# data.loc[data['Category'] == 'Политика', 'class'] = 1\n",
        "# data.loc[data['Category'] == 'Экономика', 'class'] = 2\n",
        "# data.loc[data['Category'] == 'Наука', 'class'] = 3\n",
        "# data.loc[data['Category'] == 'Статьи', 'class'] = 4\n",
        "# data.loc[data['Category'] == 'Книги', 'class'] = 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7kSUD3LG6iL"
      },
      "outputs": [],
      "source": [
        "data.to_csv('./data_lemm.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWGxTUCIG6iM"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def tfidf(texts):\n",
        "\n",
        "    corpus = texts.to_numpy()\n",
        "    \n",
        "    vocabulary = list(set([item for sublist in list(data['Text'].astype(str)) for item in sublist.split()]))\n",
        "    pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
        "                    ('tfid', TfidfTransformer())]).fit(corpus)\n",
        "    pipe['count'].transform(corpus).toarray()\n",
        "    pipe['tfid'].idf_\n",
        "\n",
        "    return pipe.transform(corpus)\n",
        "\n",
        "test = tfidf(data['Text'].astype(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYtzzBaWG6iM",
        "outputId": "02b81765-357e-4b75-a1d7-abe97b59b733"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<9096x48001 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 993973 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX1kVrlCG6iN"
      },
      "outputs": [],
      "source": [
        "def corpuses_embedding_count_vectorize(texts):\n",
        "\n",
        "    corpus = texts.to_numpy()\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    return X.toarray()\n",
        "\n",
        "test = corpuses_embedding_count_vectorize(data['Text'].astype(str))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoU9QMF8G6iN"
      },
      "outputs": [],
      "source": [
        "# [item for sublist in list(data['Text'].astype(str).values) for item in sublist.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUrbnwHPG6iO"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "def doc_to_vec(texts):\n",
        "    #tokenize and tag the card text\n",
        "    card_docs = [TaggedDocument(doc.split(' '), [i]) \n",
        "             for i, doc in enumerate(data['Text'].astype(str))]\n",
        "\n",
        "    model = Doc2Vec(vector_size=64, window=2, min_count=1, workers=8, epochs = 40)\n",
        "    #build vocab\n",
        "    model.build_vocab(card_docs)\n",
        "    #train model\n",
        "    model.train(card_docs, total_examples=model.corpus_count\n",
        "                , epochs=model.epochs)\n",
        "\n",
        "    card2vec = [model.infer_vector((texts[i].split(' '))) \n",
        "            for i in range(0,len(texts))]\n",
        "    \n",
        "    dtv= np.array(card2vec).tolist()\n",
        "\n",
        "    return dtv\n",
        "\n",
        "doc_to_vec(data['Text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJqa9UE3G6iO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7LvQfQ6G6iO"
      },
      "source": [
        "Fast Text не надо"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nza7gY3IG6iP"
      },
      "outputs": [],
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "def ft(texts):\n",
        "\n",
        "\n",
        "    embedding_size = 60\n",
        "    window_size = 40\n",
        "    min_word = 5\n",
        "    down_sampling = 1e-2\n",
        "\n",
        "    ft_model = FastText(texts,\n",
        "                        size=embedding_size,\n",
        "                        window=window_size,\n",
        "                        min_count=min_word,\n",
        "                        sample=down_sampling,\n",
        "                        sg=1,\n",
        "                        iter=1)\n",
        "    \n",
        "    return np.array([[ft_model.wv[word] for word in text] for text in data['Text']])\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Feature_extraction.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}